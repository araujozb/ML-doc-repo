{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#template-de-entrega","title":"Template de Entrega","text":"Edi\u00e7\u00e3o <p>2025.2</p>"},{"location":"#nome","title":"Nome","text":"<ul> <li>Bianca C. Fagundes de Ara\u00fajo  </li> </ul> <p>Instru\u00e7\u00f5es</p> <p>Este template ser\u00e1 utilizado como bloco de notas da disciplida de Machine Learning  e tamb\u00e9m para estudo pessoal. Este template ser\u00e1 atualizado e editado a cada entrega e registrando tudo o que foi feito at\u00e9 a ada de entrega via Git.  </p>"},{"location":"#abas","title":"Abas","text":"<p>As abas da documenta\u00e7\u00e3o s\u00e3o divididas em: - Exerc\u00edcios (registros dos exerc\u00edcios passados na mat\u00e9ria de ML) - Roteiros (resumo e estudos dos conte\u00fados englobados pela mat\u00e9ria) - Projeto ( dedicado ao Projeto Final da mat\u00e9ria de ML)  </p>"},{"location":"#entregas","title":"Entregas","text":"<ul> <li> Roteiro 1 - Data 29/08/2025</li> <li> Roteiro 2</li> <li> Roteiro 3</li> <li> Roteiro 4</li> <li> Projeto</li> </ul>"},{"location":"#diagramas","title":"Diagramas","text":"<p>Use o Mermaid para criar os diagramas de documenta\u00e7\u00e3o.</p> <p>Mermaid Live Editor</p> <pre><code>flowchart TD\n    Deployment:::orange --&gt;|defines| ReplicaSet\n    ReplicaSet --&gt;|manages| pod((Pod))\n    pod:::red --&gt;|runs| Container\n    Deployment --&gt;|scales| pod\n    Deployment --&gt;|updates| pod\n\n    Service:::orange --&gt;|exposes| pod\n\n    subgraph  \n        ConfigMap:::orange\n        Secret:::orange\n    end\n\n    ConfigMap --&gt; Deployment\n    Secret --&gt; Deployment\n    classDef red fill:#f55\n    classDef orange fill:#ffa500</code></pre>"},{"location":"#codigos","title":"C\u00f3digos","text":"De um arquivo remotoAnota\u00e7\u00f5es no c\u00f3digo main.yaml<pre><code>name: ci\non:\n  - push\n  - pull_request\n\n# Environment\nenv:\n  CI: true\n  PYTHON_VERSION: 3.12\n\n# Jobs to run\njobs:\n\n  # Build and deploy documentation site\n  deploy:\n    if: github.event_name != 'pull_request' &amp;&amp; github.ref == 'refs/heads/main'\n    runs-on: ubuntu-latest\n    steps:\n\n      # Checkout source form GitHub\n      - uses: actions/checkout@v4\n\n      # Install Python runtime and dependencies\n      - uses: actions/setup-python@v4\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n\n      # pip\n      - run: |\n          pip install -r requirements.txt\n\n      # deploy\n      - run: |\n          mkdocs gh-deploy --force\n</code></pre> compose.yaml<pre><code>name: app\n\n    db:\n        image: postgres:17\n        environment:\n            POSTGRES_DB: ${POSTGRES_DB:-projeto} # (1)!\n            POSTGRES_USER: ${POSTGRES_USER:-projeto}\n            POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-projeto}\n        ports:\n            - 5432:5432 #(2)!\n</code></pre> <ol> <li> <p>Caso a vari\u00e1vel de ambiente <code>POSTGRES_DB</code> n\u00e3o exista ou seja nula - n\u00e3o seja definida no arquivo <code>.env</code> - o valor padr\u00e3o ser\u00e1 <code>projeto</code>. Vide documenta\u00e7\u00e3o.</p> </li> <li> <p>Aqui \u00e9 feito um t\u00fanel da porta 5432 do container do banco de dados para a porta 5432 do host (no caso localhost). Em um ambiente de produ\u00e7\u00e3o, essa porta n\u00e3o deve ser exposta, pois ningu\u00e9m de fora do compose deveria acessar o banco de dados diretamente.</p> </li> </ol>"},{"location":"#exemplo-de-video","title":"Exemplo de v\u00eddeo","text":"<p>Lorem ipsum dolor sit amet</p>"},{"location":"#referencias","title":"Refer\u00eancias","text":"<p>Material for MkDocs</p>"},{"location":"decision-tree/exercicio/main/","title":"Exerc\u00edcio","text":""},{"location":"decision-tree/exercicio/main/#objetivo","title":"Objetivo","text":"<p>Aplicar o algoritmo de \u00e1rvore de decis\u00e3o em um conjunto de dados de classifica\u00e7\u00e3o, explorando e pr\u00e9-processando os dados, realizando a divis\u00e3o em treino e teste, treinando o modelo e avaliando seu desempenho por meio de m\u00e9tricas adequadas.</p>"},{"location":"decision-tree/exercicio/main/#etapas","title":"Etapas","text":"<ul> <li> Explora\u00e7\u00e3o dos Dados (EDA) </li> <li> Pr\u00e9-processamento</li> <li> Divis\u00e3o dos Dados</li> <li> Treinamento do Modelo</li> <li> Avalia\u00e7\u00e3o do Modelo</li> <li> Relat\u00f3rio Final</li> </ul>"},{"location":"decision-tree/exercicio/main/#escolha-do-dataset-mushroom-dataset","title":"Escolha do Dataset -  (Mushroom Dataset)","text":"<p>O dataset escolhido para o projeto foi o Mushroom Dataset, onde h\u00e1 as especifica\u00e7\u00f5es do cogumelo e uma coluna \"class\" que possui duas categorias ( e - eatable / p - poisonous ).  </p>"},{"location":"decision-tree/exercicio/main/#1-exploracao-dos-dados-eda","title":"1. Explora\u00e7\u00e3o dos Dados (EDA)","text":"<p>Nesta etapa, buscou-se compreender a natureza do dataset Mushroom, obtido do OpenML. Foram analisados o tamanho do conjunto, a distribui\u00e7\u00e3o da vari\u00e1vel alvo e algumas vari\u00e1veis descritivas, com apoio de estat\u00edsticas e gr\u00e1ficos.  </p> CodeOutputGr\u00e1ficoExplica\u00e7\u00e3o <pre><code>from sklearn.datasets import fetch_openml\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\n\nmush = fetch_openml(name=\"mushroom\", version=1, as_frame=True)\ndf = mush.frame\n\nprint(\"Shape:\", df.shape)\nprint(df.head())\nprint(df.describe(include='all').T.head())\nprint(\"\\nDistribui\u00e7\u00e3o da classe:\\n\", df[\"class\"].value_counts())\n\nimg_dir = \"docs/decision-tree/exercicio/img\"\nos.makedirs(img_dir, exist_ok=True)\n\ndf[\"odor\"].value_counts().plot(kind=\"bar\", title=\"Frequ\u00eancia de ODOR\")\nplt.savefig(f\"{img_dir}/eda_bar_odor.png\"); plt.clf()\n\npd.crosstab(df[\"gill-color\"], df[\"class\"]).plot(kind=\"bar\", stacked=True, title=\"Gill-color x Class\")\nplt.savefig(f\"{img_dir}/eda_stack_gillcolor.png\"); plt.clf()\n</code></pre> <pre><code>Shape: (8124, 23)\nDistribui\u00e7\u00e3o da classe:\ne    4208\np    3916\nName: class, dtype: int64\n</code></pre> <p> </p> <ul> <li>Dataset Mushroom com 8.124 amostras e 22 vari\u00e1veis categ\u00f3ricas.  </li> <li>Atributo alvo <code>class</code>: <code>e = edible (comest\u00edvel)</code> e <code>p = poisonous (venenoso)</code>.  </li> <li>Odor j\u00e1 se mostra altamente discriminativo.  </li> <li>Algumas cores de lamelas (<code>gill-color</code>) tamb\u00e9m variam fortemente por classe.  </li> </ul>"},{"location":"decision-tree/exercicio/main/#2-pre-processamento","title":"2. Pr\u00e9-processamento","text":"<p>dataset apresentou valores ausentes representados por <code>\"?\"</code>, tratados como <code>NaN</code> e posteriormente imputados pela moda. O alvo <code>class</code> foi convertido para formato bin\u00e1rio (<code>e \u2192 0</code>, <code>p \u2192 1</code>).  </p> CodeOutputExplica\u00e7\u00e3o <pre><code>import numpy as np\n\ndf.replace(\"?\", np.nan, inplace=True)\ndf = df.fillna(df.mode().iloc[0])\n\nX = df.drop(columns=[\"class\"])\ny = df[\"class\"].map({\"e\": 0, \"p\": 1})\n</code></pre> <pre><code>Nenhum valor ausente ap\u00f3s imputa\u00e7\u00e3o.\n</code></pre> <ul> <li>Substitu\u00edmos <code>?</code> por <code>NaN</code> e aplicamos imputa\u00e7\u00e3o pela moda.  </li> <li>Target <code>class</code> convertido em bin\u00e1rio (<code>e\u21920</code>, <code>p\u21921</code>).  </li> </ul>"},{"location":"decision-tree/exercicio/main/#3-divisao-dos-dados","title":"3. Divis\u00e3o dos Dados","text":"<p>As vari\u00e1veis categ\u00f3ricas foram transformadas por One-Hot Encoding, resultando em 117 colunas bin\u00e1rias. Em seguida, aplicou-se divis\u00e3o estratificada em treino (70%) e teste (30%).  </p> CodeOutputExplica\u00e7\u00e3o <pre><code>from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\n\nencoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\nX_encoded = encoder.fit_transform(X)\nX_encoded = pd.DataFrame(X_encoded, columns=encoder.get_feature_names_out(X.columns))\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X_encoded, y, test_size=0.3, stratify=y, random_state=42\n)\n</code></pre> <pre><code>X_train: (5686, 117)\nX_test:  (2438, 117)\n</code></pre> <ul> <li>One-Hot Encoding expande vari\u00e1veis categ\u00f3ricas em bin\u00e1rias.  </li> <li>Split estratificado 70/30 mant\u00e9m a propor\u00e7\u00e3o de classes.  </li> </ul>"},{"location":"decision-tree/exercicio/main/#4-treinamento-do-modelo","title":"4. Treinamento do Modelo","text":"<p>Foi utilizado o classificador <code>DecisionTreeClassifier</code> da biblioteca scikit-learn, em sua configura\u00e7\u00e3o padr\u00e3o. O modelo foi ajustado com o conjunto de treino e gerou previs\u00f5es para o conjunto de teste.</p> CodeOutputExplica\u00e7\u00e3o <pre><code>from sklearn.tree import DecisionTreeClassifier\n\nclf = DecisionTreeClassifier(random_state=42)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n</code></pre> <pre><code>Modelo DecisionTree treinado com sucesso.\n</code></pre> <ul> <li>Classificador <code>DecisionTreeClassifier</code>.  </li> <li>Treinado em <code>X_train, y_train</code>, avaliado em <code>X_test</code>.  </li> </ul>"},{"location":"decision-tree/exercicio/main/#5-avaliacao-do-modelo","title":"5. Avalia\u00e7\u00e3o do Modelo","text":"<p>O desempenho do modelo foi medido por m\u00e9tricas de acur\u00e1cia, precis\u00e3o, recall e F1-score, al\u00e9m da matriz de confus\u00e3o.  </p> CodeOutputGr\u00e1ficoExplica\u00e7\u00e3o <pre><code>from sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\n\nprint(classification_report(y_test, y_pred, target_names=[\"edible(0)\", \"poisonous(1)\"]))\n\ncm = confusion_matrix(y_test, y_pred)\nplt.imshow(cm, cmap=\"Blues\")\nplt.title(\"Matriz de Confus\u00e3o \u2014 Decision Tree\")\nplt.xticks([0,1], [\"edible(0)\", \"poisonous(1)\"])\nplt.yticks([0,1], [\"edible(0)\", \"poisonous(1)\"])\nfor i in range(cm.shape[0]):\n    for j in range(cm.shape[1]):\n        plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\nplt.xlabel(\"Predito\"); plt.ylabel(\"Real\")\nplt.savefig(\"docs/decision-tree/exercicio/img/cm_baseline.png\")\nplt.show()\n</code></pre> <pre><code>              precision    recall  f1-score   support\nedible(0)       1.00      1.00      1.00      1263\npoisonous(1)    1.00      1.00      1.00      1175\naccuracy        1.00      2438\nmacro avg       1.00      1.00      1.00      2438\nweighted avg    1.00      1.00      1.00      2438\n</code></pre> <p></p> <ul> <li>O modelo atingiu 100% de acur\u00e1cia no conjunto de teste.  </li> <li>Isso n\u00e3o \u00e9 overfitting, porque:  <ol> <li>O split foi feito corretamente (70/30, estratificado).  </li> <li>O dataset Mushroom \u00e9 determin\u00edstico: n\u00e3o h\u00e1 casos com atributos iguais mas classes diferentes.  </li> <li>Portanto, a \u00e1rvore consegue separar perfeitamente as classes sem memorizar ru\u00eddo.  </li> </ol> </li> <li>Em datasets reais, esse resultado seria suspeito, mas aqui \u00e9 esperado.  </li> </ul>"},{"location":"decision-tree/exercicio/main/#6-importancia-das-features","title":"6. Import\u00e2ncia das Features","text":"<p>Foram analisadas as vari\u00e1veis que mais contribu\u00edram para a redu\u00e7\u00e3o de impureza nos n\u00f3s da \u00e1rvore.</p> CodeOutputGr\u00e1ficoExplica\u00e7\u00e3o <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nimport os\n\nimportances = pd.Series(clf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n\ntop10 = importances.head(10)\nprint(top10)\n\nplt.figure(figsize=(10,6))\ntop10.plot(kind=\"bar\")\nplt.title(\"Top 10 Features Importantes\")\nplt.tight_layout()\nplt.savefig(\"docs/decision-tree/exercicio/img/feature_importances.png\")\nplt.show()\n</code></pre> <pre><code>odor_n                 0.89\nspore-print-color_r    0.04\ngill-size_b            0.03\nstalk-root_b           0.02\n...\n</code></pre> <p></p> <ul> <li>O atributo odor \u00e9 de longe o mais importante para a classifica\u00e7\u00e3o.  </li> <li>Outros atributos como spore-print-color e gill-size tamb\u00e9m contribuem.  </li> <li>Features com import\u00e2ncia pr\u00f3xima de zero n\u00e3o foram usadas na \u00e1rvore.  </li> </ul>"},{"location":"decision-tree/exercicio/main/#7-visualizacao-da-arvore","title":"7. Visualiza\u00e7\u00e3o da \u00c1rvore","text":"<p>Para melhor interpretabilidade, foi gerada uma visualiza\u00e7\u00e3o dos quatro primeiros n\u00edveis da \u00e1rvore, evitando excesso de ramifica\u00e7\u00f5es.  </p> CodeOutput\u00c1rvoreExplica\u00e7\u00e3o <pre><code>from sklearn.tree import plot_tree\n\nplt.figure(figsize=(20, 10))\nplot_tree(\n    clf,\n    feature_names=X_train.columns,\n    class_names=[\"edible(0)\", \"poisonous(1)\"],\n    filled=True, rounded=True, fontsize=8,\n    max_depth=4\n)\nplt.savefig(\"docs/decision-tree/exercicio/img/tree_top.png\")\nplt.show()\n</code></pre> <pre><code>Figura salva em: docs/decision-tree/exercicio/img/tree_top.png\n</code></pre> <p></p> <ul> <li>Mostramos apenas os 4 primeiros n\u00edveis da \u00e1rvore para clareza.  </li> <li>A raiz \u00e9 dominada por vari\u00e1veis de odor, confirmando sua relev\u00e2ncia.  </li> <li>A \u00e1rvore completa \u00e9 muito maior devido ao One-Hot Encoding.  </li> </ul>"},{"location":"decision-tree/exercicio/main/#8-conclusoes","title":"8. Conclus\u00f5es","text":"<ul> <li>O modelo obteve 100% de acur\u00e1cia, mas isso n\u00e3o \u00e9 overfitting, e sim reflexo de um dataset sem ru\u00eddo e determin\u00edstico.  </li> <li>O pr\u00e9-processamento simples (imputa\u00e7\u00e3o da moda + One-Hot Encoding) foi suficiente.  </li> <li>As vari\u00e1veis mais importantes confirmam expectativas biol\u00f3gicas (ex.: odor como crit\u00e9rio principal).  </li> <li>A \u00e1rvore de decis\u00e3o mostrou-se totalmente interpret\u00e1vel, atendendo ao objetivo do exerc\u00edcio.</li> </ul>"},{"location":"decision-tree/roteiro/main/","title":"Roteiro","text":""},{"location":"kmeans/exercicio/main/","title":"Exerc\u00edcio","text":""},{"location":"kmeans/exercicio/main/#objetivo","title":"Objetivo","text":"<p>Aplicar o algoritmo K-Means em um conjunto de dados para realizar segmenta\u00e7\u00e3o de clientes. Para tanto, foi alterado o dataset original (Mushroom), utilizando ent\u00e3o o  Mall Customers Dataset (Customer Segmentation).</p>"},{"location":"kmeans/exercicio/main/#por-que-o-dataset-foi-alterado","title":"Por que o Dataset foi alterado?","text":"<p>O Mushroom Dataset \u00e9 totalmente categ\u00f3rico, todas as 22 features s\u00e3o categ\u00f3ricas e n\u00e3o num\u00e9ricas.</p> <p>O K-Means n\u00e3o lida bem com vari\u00e1veis categ\u00f3ricas, por que ele usa dist\u00e2ncia euclidiana, que por sua vez, s\u00f3 funciona bem com vari\u00e1veis cont\u00ednuas.</p> <p>Outro ponto \u00e9 que o Mush \u00e9 100% separ\u00e1vel pela classe (eatible x poisonous), isso diz que o dataset j\u00e1 possui uma separa\u00e7\u00e3o perfeira, ent\u00e3o segmentar com K-Means n\u00e3o encontra novos grupos.</p>"},{"location":"kmeans/exercicio/main/#1-exploracao-dos-dados-eda","title":"1. Explora\u00e7\u00e3o dos Dados (EDA)","text":"<p>O conjunto de dados Mall Customers \u00e9 composto por 200 clientes de um shopping center, com as seguintes vari\u00e1veis: CustomerID (identificador), Gender (g\u00eanero), Age (idade), Annual Income ($) (renda anual em milhares de d\u00f3lares) e Spending Score (1\u2013100) (pontua\u00e7\u00e3o atribu\u00edda pelo shopping com base no comportamento de compra).</p> <p>A an\u00e1lise explorat\u00f3ria inicial mostrou que n\u00e3o h\u00e1 valores ausentes nas vari\u00e1veis. As distribui\u00e7\u00f5es indicam que a idade dos clientes est\u00e1 concentrada entre aproximadamente 20 e 50 anos, enquanto a renda anual varia em torno de 15 a 140 mil d\u00f3lares. O Spending Score apresenta uma distribui\u00e7\u00e3o bastante espalhada, o que sugere perfis de consumo bem diferentes dentro da base. A vari\u00e1vel Gender est\u00e1 relativamente equilibrada entre clientes do sexo masculino e feminino.</p> <p>Os gr\u00e1ficos de dispers\u00e3o entre idade, renda e Spending Score indicam que n\u00e3o existe uma rela\u00e7\u00e3o linear forte entre essas vari\u00e1veis, o que refor\u00e7a a necessidade de usar t\u00e9cnicas de agrupamento para identificar segmentos de clientes com padr\u00f5es de comportamento semelhantes.</p> Gr\u00e1fico <p> </p>"},{"location":"kmeans/exercicio/main/#2-pre-processamento","title":"2. Pr\u00e9-processamento","text":"<p>O dataset Mall Customers apresentou-se inicialmente sem valores ausentes, garantindo boa qualidade dos dados para processamento. Como primeira etapa, a coluna CustomerID foi removida por se tratar apenas de um identificador e n\u00e3o conter informa\u00e7\u00e3o relevante para o processo de clusteriza\u00e7\u00e3o.</p> <p>A vari\u00e1vel categ\u00f3rica Gender foi convertida em formato num\u00e9rico, atribuindo-se o valor 0 para Male e 1 para Female. Essa transforma\u00e7\u00e3o \u00e9 necess\u00e1ria, pois o algoritmo K-Means opera exclusivamente com vari\u00e1veis num\u00e9ricas.</p> <p>As features selecionadas para o modelo foram:</p> <ul> <li>Gender_num</li> <li>Age</li> <li>Annual Income (k$)</li> <li>Spending Score (1\u2013100)</li> </ul> <p>Por fim, todas as vari\u00e1veis foram padronizadas utilizando o m\u00e9todo StandardScaler, de forma que cada feature apresentasse m\u00e9dia 0 e desvio padr\u00e3o 1. Essa normaliza\u00e7\u00e3o \u00e9 fundamental, uma vez que o K-Means utiliza dist\u00e2ncia Euclidiana e seria fortemente influenciado por vari\u00e1veis com escalas maiores, como a renda anual.</p>"},{"location":"kmeans/exercicio/main/#3-divisao-dos-dados","title":"3. Divis\u00e3o dos Dados","text":"<p>Embora o K-Means seja um algoritmo de aprendizado n\u00e3o supervisionado, a rubrica da atividade exige a separa\u00e7\u00e3o dos dados em treino e teste.  </p> <p>Assim, a matriz de features padronizadas foi dividida em:</p> <ul> <li>80% para treino </li> <li>20% para teste</li> </ul> <p>O objetivo dessa etapa \u00e9 verificar a estabilidade dos clusters quando aplicados a um subconjunto de dados n\u00e3o utilizado na fase inicial de ajuste.</p> <p>Essa divis\u00e3o n\u00e3o afeta a constru\u00e7\u00e3o dos clusters, mas auxilia na avalia\u00e7\u00e3o da consist\u00eancia do modelo.</p>"},{"location":"kmeans/exercicio/main/#4-treinamento-do-modelo","title":"4. Treinamento do Modelo","text":""},{"location":"kmeans/exercicio/main/#41-metodo-do-cotovelo","title":"4.1 M\u00e9todo do Cotovelo","text":"<p>Para determinar o n\u00famero ideal de clusters, aplicou-se o m\u00e9todo do cotovelo, que consiste em calcular a in\u00e9rcia (WCSS \u2013 Within Cluster Sum of Squares) para diferentes valores de k.</p> <p>O gr\u00e1fico obtido mostra uma queda acentuada da in\u00e9rcia entre k = 1 e k = 4. A partir de k = 5, a redu\u00e7\u00e3o passa a ser marginal, caracterizando o ponto de  diminui\u00e7\u00e3o dos ganhos, conhecido como \u201ccotovelo\u201d.</p> <p>Com base nessa an\u00e1lise, o n\u00famero de clusters mais apropriado para este dataset situa-se entre 4 e 5. Nos passos seguintes, utilizaremos k = 5, pois esse valor costuma gerar segmentos mais \u00fateis e facilmente interpret\u00e1veis.</p> Gr\u00e1fico <p> </p>"},{"location":"kmeans/exercicio/main/#42-interpretacao-do-silhouette-score","title":"4.2 Interpreta\u00e7\u00e3o do Silhouette Score","text":"<p>Ap\u00f3s o ajuste do modelo K-Means para diferentes valores de k (entre 2 e 7),  foi calculado o Silhouette Score, m\u00e9trica que avalia simultaneamente:</p> <ul> <li>coes\u00e3o interna, ou seja, qu\u00e3o pr\u00f3ximos os pontos est\u00e3o do seu pr\u00f3prio cluster;  </li> <li>separa\u00e7\u00e3o, isto \u00e9, qu\u00e3o distantes est\u00e3o dos clusters vizinhos.</li> </ul> <p>O Silhouette varia entre -1 e 1. Valores pr\u00f3ximos de 1 indicam clusters bem  separados; valores pr\u00f3ximos de 0 indicam sobreposi\u00e7\u00e3o entre grupos; e valores  negativos sugerem agrupamentos inadequados.</p> <p>Os resultados obtidos foram: | K | Silhouette Treino | Silhouette Teste | |---|-------------------|------------------| | 2 | 0.2467 | 0.2619 | | 3 | 0.2466 | 0.2040 | | 4 | 0.2951 | 0.2333 | | 5 | 0.3103 | 0.2260 | | 6 | 0.3262 | 0.2962 | | 7 | 0.3668 | 0.2998 |</p> <p>Embora esses valores n\u00e3o sejam elevados (idealmente entre 0.4 e 0.5), eles se  enquadram dentro do esperado para problemas reais de segmenta\u00e7\u00e3o, nos quais  os perfis dos clientes tendem a apresentar transi\u00e7\u00f5es suaves e n\u00e3o  fronteiras r\u00edgidas. Em bases de dados comportamentais, \u00e9 comum que diferentes  grupos de consumidores possuam caracter\u00edsticas parcialmente sobrepostas, o que  naturalmente reduz o Silhouette Score.</p> <p>Entre todos os valores testados, o maior Silhouette foi obtido com k = 7,  tanto no conjunto de treino quanto no teste. Isso indica que esse valor oferece  o melhor equil\u00edbrio entre coes\u00e3o e separa\u00e7\u00e3o dos clusters, representando o  n\u00famero de segmentos que mais adequadamente descreve a estrutura presente nos  dados.</p> <p>Assim, o modelo final adotado foi o K-Means com 7 clusters, refletindo uma  segmenta\u00e7\u00e3o mais granular e informativa dos perfis de clientes.</p>"},{"location":"kmeans/exercicio/main/#43-treinamento-final-do-modelo-k-means","title":"4.3 Treinamento Final do Modelo K-Means","text":"<p>Embora o M\u00e9todo do Cotovelo tenha sugerido um intervalo poss\u00edvel entre k = 4 e k = 5, a valida\u00e7\u00e3o por meio do Silhouette Score demonstrou que o valor de k = 7 apresenta a melhor combina\u00e7\u00e3o entre coes\u00e3o interna e separa\u00e7\u00e3o entre os clusters.</p> <p>Assim, o modelo final foi treinado com 7 clusters, utilizando 80% dos dados (normalizados) e o par\u00e2metro <code>n_init=10</code>, garantindo maior estabilidade na inicializa\u00e7\u00e3o dos centr\u00f3ides.</p> <p>Ap\u00f3s o ajuste, cada cliente foi atribu\u00eddo a um dos sete clusters, tanto no conjunto de treino quanto no teste, permitindo analisar a consist\u00eancia e o perfil dos segmentos identificados.</p>"},{"location":"kmeans/exercicio/main/#5-interpretacao-dos-clusters-k-7","title":"5. Interpreta\u00e7\u00e3o dos Clusters (k = 7)","text":"<p>Ap\u00f3s o treinamento final do modelo K-Means com k = 7, foi realizada uma an\u00e1lise detalhada das caracter\u00edsticas m\u00e9dias de cada grupo. A tabela de perfis permite compreender o comportamento dos segmentos com base em quatro vari\u00e1veis: g\u00eanero, idade, renda anual e spending score.</p> <p>A seguir, apresenta-se a interpreta\u00e7\u00e3o dos clusters:</p> <ul> <li> <p>Cluster 0 \u2014 Mulheres maduras conservadoras: composto exclusivamente por   mulheres com idade m\u00e9dia superior a 50 anos, renda intermedi\u00e1ria e gasto   baixo. Representa clientes de perfil mais est\u00e1vel e pouco engajado.</p> </li> <li> <p>Cluster 1 \u2014 Homens maduros conservadores: grupo masculino de maior idade   m\u00e9dia (56 anos), renda mediana e baixo consumo. Similar ao cluster 0, por\u00e9m   no p\u00fablico masculino.</p> </li> <li> <p>Cluster 2 \u2014 Jovens de alta renda \u2013 Gastadores (VIP masculino): formado por   homens de cerca de 33 anos, com alta renda (\u224887k) e elevado spending score.   Segmento premium e altamente valioso.</p> </li> <li> <p>Cluster 3 \u2014 Mulheres jovens engajadas: mulheres por volta de 26 anos com   renda mais baixa, por\u00e9m gasto acima da m\u00e9dia. Perfil impulsivo, responde bem   a promo\u00e7\u00f5es.</p> </li> <li> <p>Cluster 4 \u2014 Homens jovens gastadores: jovens de baixa renda (\u224840k), mas   com spending score elevado. Segmento com comportamento de compra semelhante   ao cluster 3, por\u00e9m masculino.</p> </li> <li> <p>Cluster 5 \u2014 Mulheres de alta renda \u2013 Gastadoras (VIP feminino): renda alta   (\u224886k), spending score muito elevado e idade m\u00e9dia de 32 anos. Grupo de grande   valor comercial, ideal para a\u00e7\u00f5es premium e fideliza\u00e7\u00e3o.</p> </li> <li> <p>Cluster 6 \u2014 Alta renda, baixo engajamento: grupo misto em g\u00eanero, com a   maior renda m\u00e9dia entre todos os clusters (\u224892k), por\u00e9m com gasto muito reduzido.   Indica clientes de alto potencial n\u00e3o aproveitado.</p> </li> </ul> <p>Esses segmentos oferecem uma vis\u00e3o granular e estrat\u00e9gica do comportamento dos clientes, permitindo a\u00e7\u00f5es direcionadas de marketing, reten\u00e7\u00e3o e fideliza\u00e7\u00e3o.</p>"},{"location":"kmeans/exercicio/main/#4-treinamento-do-modelo_1","title":"4. Treinamento do Modelo","text":"<p>O modelo K-Nearest Neighbors (KNN) foi treinado com diferentes valores de k (1, 3, 5, 7, 9, 11), buscando identificar o n\u00famero ideal de vizinhos que maximiza o desempenho.</p> <p>Como o algoritmo \u00e9 baseado em dist\u00e2ncias, o One-Hot Encoding aplicado anteriormente garante que as categorias sejam interpretadas de forma bin\u00e1ria e equidistante, evitando distor\u00e7\u00f5es no c\u00e1lculo da similaridade entre amostras.</p>"},{"location":"kmeans/exercicio/main/#5-avaliacao-do-modelo","title":"5. Avalia\u00e7\u00e3o do Modelo","text":"<p>Foram calculadas m\u00e9tricas de acur\u00e1cia, precis\u00e3o, recall e F1-score, al\u00e9m da matriz de confus\u00e3o.</p> <p>Como no modelo anterior (\u00e1rvore de decis\u00e3o), o KNN tamb\u00e9m atingiu desempenho m\u00e1ximo \u2014 o que refor\u00e7a a natureza determin\u00edstica do dataset Mushroom. </p>"},{"location":"kmeans/exercicio/main/#6-conclusao","title":"6. Conclus\u00e3o","text":"<p>O objetivo deste trabalho foi aplicar o algoritmo K-Means em um problema de  segmenta\u00e7\u00e3o de clientes, utilizando o Mall Customers Dataset em substitui\u00e7\u00e3o ao conjunto de dados original (Mushroom). Essa altera\u00e7\u00e3o permitiu trabalhar com  vari\u00e1veis num\u00e9ricas cont\u00ednuas, mais adequadas ao uso da dist\u00e2ncia euclidiana, que  \u00e9 a base de funcionamento do K-Means.</p> <p>A partir da an\u00e1lise explorat\u00f3ria, foi poss\u00edvel observar que idade, renda anual e  Spending Score apresentam grande variabilidade e n\u00e3o exibem rela\u00e7\u00f5es lineares  fortes entre si. Isso refor\u00e7a a escolha do K-Means como t\u00e9cnica de agrupamento, j\u00e1 que o objetivo n\u00e3o \u00e9 prever um r\u00f3tulo, mas sim identificar padr\u00f5es de  comportamento em uma base heterog\u00eanea de clientes. O pr\u00e9-processamento incluiu  a remo\u00e7\u00e3o do identificador (CustomerID), a codifica\u00e7\u00e3o da vari\u00e1vel categ\u00f3rica  (Gender) e a padroniza\u00e7\u00e3o das features, garantindo que todas contribu\u00edssem de  forma equilibrada para o c\u00e1lculo das dist\u00e2ncias.</p> <p>Na etapa de modelagem, o M\u00e9todo do Cotovelo foi utilizado para sugerir um intervalo  inicial plaus\u00edvel para o n\u00famero de clusters, enquanto o Silhouette Score foi  empregado como crit\u00e9rio quantitativo para comparar diferentes valores de k. Os  melhores resultados foram obtidos com k = 7, que apresentou os maiores valores  de Silhouette tanto em treino (0,3668) quanto em teste (0,2998). Embora esses  valores n\u00e3o sejam elevados, eles s\u00e3o compat\u00edveis com problemas reais de  segmenta\u00e7\u00e3o, nos quais os grupos tendem a se sobrepor parcialmente.</p> <p>A interpreta\u00e7\u00e3o dos sete clusters revelou perfis de clientes claramente distintos,  como grupos de alta renda e alto gasto (segmentos VIP), jovens de baixa renda  com alto Spending Score e um segmento de alta renda com baixo engajamento,  que representa potencial de crescimento ainda n\u00e3o explorado. Esses insights podem  ser utilizados para orientar a\u00e7\u00f5es de marketing mais direcionadas, programas de  fideliza\u00e7\u00e3o e estrat\u00e9gias de reten\u00e7\u00e3o de clientes.</p> <p>Como trabalhos futuros, seria interessante: - incorporar novas vari\u00e1veis comportamentais (frequ\u00eancia de visitas, categorias de produtos, canal de compra, etc.); - comparar o desempenho do K-Means com outros algoritmos de clusteriza\u00e7\u00e3o    (como DBSCAN, GMM ou m\u00e9todos hier\u00e1rquicos); - analisar a evolu\u00e7\u00e3o dos clusters ao longo do tempo, avaliando mudan\u00e7as no    comportamento dos segmentos identificados.</p> <p>Em s\u00edntese, o uso do K-Means sobre o Mall Customers Dataset mostrou-se adequado  para o problema proposto, permitindo identificar segmentos relevantes de clientes  e demonstrando, na pr\u00e1tica, a aplica\u00e7\u00e3o de t\u00e9cnicas de aprendizado n\u00e3o  supervisionado em um contexto real de neg\u00f3cios.</p>"},{"location":"kmeans/roteiro/main/","title":"Roteiro","text":""},{"location":"knn/exercicio/main/","title":"Exerc\u00edcio","text":""},{"location":"knn/exercicio/main/#objetivo","title":"Objetivo","text":"<p>Aplicar o algoritmo K-Nearest Neighbors (KNN) em um conjunto de dados de classifica\u00e7\u00e3o, utilizando a mesma base de dados empregada no experimento anterior com \u00c1rvore de Decis\u00e3o (Decision Tree) \u2014 o Mushroom Dataset.</p> <p>O objetivo \u00e9 comparar o desempenho entre os dois algoritmos, reutilizando o mesmo processo de explora\u00e7\u00e3o e pr\u00e9-processamento, para avaliar diferen\u00e7as de comportamento e de interpreta\u00e7\u00e3o entre modelos baseados em regras (Decision Tree) e modelos baseados em dist\u00e2ncia (KNN).</p>"},{"location":"knn/exercicio/main/#etapas","title":"Etapas","text":"<ul> <li> Explora\u00e7\u00e3o dos Dados (EDA) </li> <li> Pr\u00e9-processamento</li> <li> Divis\u00e3o dos Dados</li> <li> Treinamento do Modelo</li> <li> Avalia\u00e7\u00e3o do Modelo</li> <li> Relat\u00f3rio Final</li> </ul>"},{"location":"knn/exercicio/main/#escolha-do-dataset-mushroom-dataset","title":"Escolha do Dataset -  (Mushroom Dataset)","text":"<p>O mesmo dataset utilizado no projeto de \u00c1rvore de Decis\u00e3o foi mantido aqui, para permitir compara\u00e7\u00e3o direta entre os algoritmos. O Mushroom Dataset, obtido no OpenML, descrede 8.124 cogumeos com 22 atributos categ\u00f3ricos (ex.: odor, cor das lamelas, textura do chap\u00e9u) e uma vari\u00e1vel-alvo \"class\" com duas categorias:     - e: edible (comest\u00edvel)     - p: poisonous (venenoso)</p>"},{"location":"knn/exercicio/main/#1-exploracao-dos-dados-eda","title":"1. Explora\u00e7\u00e3o dos Dados (EDA)","text":"<p>A etapa de explora\u00e7\u00e3o foi reaproveitada integralmente do projeto anterior, visto que as estat\u00edsticas permanecem as mesmas.</p> Gr\u00e1ficoExplica\u00e7\u00e3o <p> </p> <ul> <li>Dataset Mushroom com 8.124 amostras e 22 vari\u00e1veis categ\u00f3ricas.  </li> <li>Atributo alvo <code>class</code>: <code>e = edible (comest\u00edvel)</code> e <code>p = poisonous (venenoso)</code>.  </li> <li>Odor j\u00e1 se mostra altamente discriminativo.  </li> <li>Algumas cores de lamelas (<code>gill-color</code>) tamb\u00e9m variam fortemente por classe.  </li> </ul>"},{"location":"knn/exercicio/main/#2-pre-processamento","title":"2. Pr\u00e9-processamento","text":"<p>Assim como no projeto anterior, o dataset apresentava valores ausentes representados por \"?\", tratados como NaN e imputados pela moda.</p> <p>As vari\u00e1veis categ\u00f3ricas foram codificadas com One-Hot Encoding, tamb\u00e9m reaproveitando a mesma estrat\u00e9gia usada na \u00e1rvore de decis\u00e3o.</p> <p>A principal diferen\u00e7a \u00e9 conceitual:</p> <pre><code>- Na **\u00c1rvore de Decis\u00e3o**, o One-Hot era opcional (o modelo lida bem com categorias).\n- No **KNN**, o One-Hot \u00e9 essencial, pois o algoritmo depende de dist\u00e2ncias num\u00e9ricas.\n</code></pre> CodeOutputExplica\u00e7\u00e3o <pre><code>import numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\n\ndf.replace(\"?\", np.nan, inplace=True)\ndf = df.fillna(df.mode().iloc[0])\n\nX = df.drop(columns=[\"class\"])\ny = df[\"class\"].map({\"e\": 0, \"p\": 1})\n\nencoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\nX_encoded = encoder.fit_transform(X)\nX_encoded = pd.DataFrame(X_encoded, columns=encoder.get_feature_names_out(X.columns))\n</code></pre> <pre><code>Nenhum valor ausente ap\u00f3s imputa\u00e7\u00e3o.\n</code></pre> <ul> <li>Mesmo tratamento de dados usado na \u00c1rvore de Decis\u00e3o, garantindo comparabilidade.  </li> <li>One-Hot Encoding necess\u00e1rio para o KNN medir dist\u00e2ncias corretamente.</li> <li>Target convertido em bin\u00e1rio (e\u21920, p\u21921), mantendo o mesmo mapeamento anterior.</li> </ul>"},{"location":"knn/exercicio/main/#3-divisao-dos-dados","title":"3. Divis\u00e3o dos Dados","text":"<p>Os dados foram divididos em conjuntos de treino (70%) e teste (30%) de forma estratificada, preservando a propor\u00e7\u00e3o entre as classes.</p> Code <p>```python</p> <p>from sklearn.model_selection import train_test_split</p> <pre><code>X_train, X_test, y_train, y_test = train_test_split(\n    X_encoded, y, test_size=0.3, stratify=y, random_state=42\n)\n\nprint(\"X_train:\", X_train.shape)\nprint(\"X_test:\", X_test.shape)\n```\n</code></pre> OutputExplica\u00e7\u00e3o <pre><code>X_train: (5686, 117)\nX_test:  (2438, 117)\n</code></pre> <ul> <li>Divis\u00e3o 70/30 com estratifica\u00e7\u00e3o, mantendo a propor\u00e7\u00e3o original das classes</li> <li>O One-Hot Encoding aplicado previamente garante que todas as vari\u00e1veis categ\u00f3ricas sejam tratadas como vetores bin\u00e1rios, permitindo o c\u00e1lculo correto das dist\u00e2ncias pelo KNN.</li> <li>Essa abordagem preserva a coer\u00eancia experimental em rela\u00e7\u00e3o ao projeto anterior de \u00c1rvore de Decis\u00e3o \u2014 apenas o algoritmo de modelagem muda, n\u00e3o o tratamento dos dados.</li> </ul>"},{"location":"knn/exercicio/main/#4-treinamento-do-modelo","title":"4. Treinamento do Modelo","text":"<p>O modelo K-Nearest Neighbors (KNN) foi treinado com diferentes valores de k (1, 3, 5, 7, 9, 11), buscando identificar o n\u00famero ideal de vizinhos que maximiza o desempenho.</p> <p>Como o algoritmo \u00e9 baseado em dist\u00e2ncias, o One-Hot Encoding aplicado anteriormente garante que as categorias sejam interpretadas de forma bin\u00e1ria e equidistante, evitando distor\u00e7\u00f5es no c\u00e1lculo da similaridade entre amostras.</p> CodeOutputExplica\u00e7\u00e3o <pre><code>from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\nmelhor_k = None\nmelhor_acc = 0.0\n\nfor k in [1, 3, 5, 7, 9, 11]:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    acc = accuracy_score(y_test, knn.predict(X_test))\n    print(f\"k={k} \u2192 accuracy={acc:.4f}\")\n    if acc &gt; melhor_acc:\n        melhor_acc = acc\n        melhor_k = k\n\nprint(f\"\\nMelhor K encontrado: {melhor_k} com acur\u00e1cia de {melhor_acc:.4f}\")\n</code></pre> <pre><code>k=1 \u2192 accuracy=1.0000\nk=3 \u2192 accuracy=1.0000\nk=5 \u2192 accuracy=1.0000\nk=7 \u2192 accuracy=1.0000\nk=9 \u2192 accuracy=0.9992\nk=11 \u2192 accuracy=0.9992\nMelhor K encontrado: 1 com acur\u00e1cia de 1.0000\n</code></pre> <ul> <li>O desempenho manteve-se est\u00e1vel entre k=1 e k=7.</li> <li>Foi adotado k = 3, o menor valor \u00edmpar com acur\u00e1cia m\u00e1xima e menor vari\u00e2ncia.</li> <li>O modelo com k=3 servir\u00e1 como base para a avalia\u00e7\u00e3o final. </li> </ul>"},{"location":"knn/exercicio/main/#5-avaliacao-do-modelo","title":"5. Avalia\u00e7\u00e3o do Modelo","text":"<p>Foram calculadas m\u00e9tricas de acur\u00e1cia, precis\u00e3o, recall e F1-score, al\u00e9m da matriz de confus\u00e3o.</p> <p>Como no modelo anterior (\u00e1rvore de decis\u00e3o), o KNN tamb\u00e9m atingiu desempenho m\u00e1ximo \u2014 o que refor\u00e7a a natureza determin\u00edstica do dataset Mushroom. </p> CodeOutputExplica\u00e7\u00e3o <pre><code>    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n\nknn_final = KNeighborsClassifier(n_neighbors=3)\nknn_final.fit(X_train, y_train)\ny_pred = knn_final.predict(X_test)\n\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nprint(f\"Accuracy={acc:.4f} Precision={prec:.4f} Recall={rec:.4f} F1={f1:.4f}\")\n\ncm = confusion_matrix(y_test, y_pred)\nConfusionMatrixDisplay(cm, display_labels=[\"edible(0)\", \"poisonous(1)\"]).plot()\nplt.title(\"Matriz de Confus\u00e3o \u2014 KNN (k=3)\")\nplt.tight_layout()\nplt.savefig(\"docs/knn/exercicio/img/confusion_matrix_k3.png\", dpi=150)\nplt.show()\n</code></pre> <pre><code>Accuracy=1.0000  Precision=1.0000  Recall=1.0000  F1=1.0000\n</code></pre> <ul> <li>O modelo atingiu 100% de acur\u00e1cia no conjunto de teste.  </li> <li>N\u00e3o houve falsos positivos nem falsos negativos. </li> <li>O desempenho \u00e9 id\u00eantico ao da \u00c1rvore de Decis\u00e3o, confirmando a separabilidade perfeita do dataset. </li> </ul>"},{"location":"knn/exercicio/main/#6-visualizacao-do-limite-de-decisao-pca","title":"6. Visualiza\u00e7\u00e3o do Limite de Decis\u00e3o (PCA)","text":"<p>Para representar graficamente as regi\u00f5es de decis\u00e3o do modelo, aplicou-se PCA (An\u00e1lise de Componentes Principais), reduzindo as 117 dimens\u00f5es (geradas pelo One-Hot Encoding) para apenas 2 componentes.</p> <p>Essa proje\u00e7\u00e3o \u00e9 apenas visual \u2014 o modelo original continua sendo treinado com todas as vari\u00e1veis.</p> CodeGr\u00e1ficoExplica\u00e7\u00e3o <pre><code>    from sklearn.decomposition import PCA\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from sklearn.neighbors import KNeighborsClassifier\n\n    pca = PCA(n_components=2, random_state=42)\n    X_train_2d = pca.fit_transform(X_train)\n    knn_2d = KNeighborsClassifier(n_neighbors=3).fit        (X_train_2d, y_train)\n\n    h = 0.05\n    x_min, x_max = X_train_2d[:,0].min()-1, X_train_2d[:,0].max     ()+1\n    y_min, y_max = X_train_2d[:,1].min()-1, X_train_2d[:,1].max     ()+1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange      (y_min, y_max, h))\n\n    Z = knn_2d.predict(np.c_[xx.ravel(), yy.ravel()]).reshape       (xx.shape)\n    plt.figure(figsize=(8,6))\n    plt.contourf(xx, yy, Z, alpha=0.3)\n    plt.scatter(X_train_2d[:,0], X_train_2d[:,1], c=y_train,        s=20, edgecolor=\"k\")\n    plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\n    plt.title(\"KNN Decision Boundary (PCA-2D, k=3)\")\n    plt.tight_layout()\n    plt.savefig(\"docs/knn/exercicio/img/        decision_boundary_pca2d.png\", dpi=150)\n</code></pre> <p></p> <ul> <li>As \u00e1reas coloridas representam regi\u00f5es de decis\u00e3o baseadas na dist\u00e2ncia aos vizinhos mais pr\u00f3ximos.</li> <li>O gr\u00e1fico mostra uma separa\u00e7\u00e3o n\u00edtida entre as classes projetadas no plano 2D.</li> <li>O uso de PCA permite visualizar o comportamento de um modelo de alta dimensionalidade. </li> </ul>"},{"location":"knn/exercicio/main/#8-conclusoes","title":"8. Conclus\u00f5es","text":"<p>O modelo KNN alcan\u00e7ou desempenho m\u00e1ximo (100%), assim como a \u00c1rvore de Decis\u00e3o. Esse resultado confirma que o dataset Mushroom \u00e9 determin\u00edstico, ou seja, n\u00e3o cont\u00e9m amostras id\u00eanticas com classes diferentes.</p> <p>O One-Hot Encoding teve papel essencial no sucesso do modelo, pois possibilitou que o KNN interpretasse corretamente as categorias durante o c\u00e1lculo de dist\u00e2ncias.</p> <p>A visualiza\u00e7\u00e3o via PCA evidenciou a clara separa\u00e7\u00e3o entre classes, refor\u00e7ando a consist\u00eancia do resultado.</p> <p>Conclus\u00e3o: ambos os modelos (Decision Tree e KNN) s\u00e3o capazes de resolver perfeitamente esse problema, mas a \u00c1rvore de Decis\u00e3o \u00e9 mais interpret\u00e1vel, enquanto o KNN \u00e9 mais sens\u00edvel a redund\u00e2ncias e ao volume de dados.</p>"},{"location":"projeto/main/","title":"Projeto","text":"<p>Aqui vai toda a documenta\u00e7\u00e3o do projeto, incluindo o que j\u00e1 foi feito e o que falta fazer.</p>"},{"location":"roteiro2/main/","title":"Main","text":""},{"location":"roteiro2/main/#diagrama-de-classes-do-banco","title":"Diagrama de Classes do Banco","text":"<pre><code>classDiagram\n    class Conta {\n        - String id\n        # double saldo\n        - Cliente cliente\n        + sacar(double valor)\n        + depositar(double valor)\n    }\n    class Cliente {\n        - String id\n        - String nome\n        - List&lt;Conta&gt; contas\n    }\n    class PessoaFisica {\n        - String cpf\n    }\n    class PessoaJuridica {\n        - String cnpj\n    }\n    class ContaCorrente {\n        - double limite\n        + sacar(double valor)\n    }\n    class ContaPoupanca {\n        + sacar(double valor)\n    }\n    Conta *-- Cliente\n    Conta &lt;|-- ContaCorrente\n    Conta &lt;|-- ContaPoupanca\n    Cliente &lt;|-- PessoaFisica\n    Cliente &lt;|-- PessoaJuridica</code></pre>"},{"location":"roteiro2/main/#diagrama-de-sequencia-de-autorizacao","title":"Diagrama de Seq\u00fc\u00eancia de Autoriza\u00e7\u00e3o","text":"<pre><code>sequenceDiagram\n  autonumber\n  actor User\n  User-&gt;&gt;Auth Service: request with token\n  Auth Service-&gt;&gt;Auth Service: decodes the token and extracts claims\n  Auth Service-&gt;&gt;Auth Service: verifies permissions\n  critical allowed\n    Auth Service-&gt;&gt;Secured Resource: authorizes the request\n    Secured Resource-&gt;&gt;User: returns the response\n  option denied\n    Auth Service--&gt;&gt;User: unauthorized message\n  end  </code></pre>"},{"location":"roteiro3/main/","title":"Main","text":"<p>Running the code below in Browser (Woooooowwwwww!!!!!!). <sup>1</sup></p> <p> </p> Editor (session: default) Run <pre>import ssl\nimport pandas as pd\n\ndf = pd.DataFrame()\ndf['AAPL'] = pd.Series([1, 2, 3])\ndf['MSFT'] = pd.Series([4, 5, 6])\ndf['GOOGL'] = pd.Series([7, 8, 9])\n\nprint(df)\n</pre> Output Clear <pre></pre> <p></p> <ol> <li> <p>Pyodide \u21a9</p> </li> </ol>"},{"location":"roteiro4/main/","title":"Main","text":"<p>Se chegou aqui, \u00e9 porque voc\u00ea est\u00e1 interessado em saber mais. Logo, de brinde, como rodar um c\u00f3digo <code>Python</code> aqui.</p> 2025-12-02T21:56:58.423902 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ 2025-12-02T21:56:59.794307 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Markdown-exec \u00e9 uma extens\u00e3o do Markdown que permite executar c\u00f3digo Python diretamente no Markdown. Isso \u00e9 \u00fatil para gerar resultados din\u00e2micos ou executar scripts de forma interativa.</p>"}]}